
		From Wikipedia, the free encyclopedia
		
		
		
		
		Jump to navigation
		Jump to search
		.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}Part of a series onMachine learningand data mining
Problems
Classification
Clustering
Regression
Anomaly detection
Data Cleaning
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction

Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)

Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t-SNE

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
k-NN
Local outlier factor

Artificial neural network
Autoencoder
Cognitive computing
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
ESN
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net
Transformer
Vision
Spiking neural network
Memtransistor
Electrochemical RAM (ECRAM)

Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)

Theory
Kernel machines
Bias–variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory

Machine-learning venues
NeurIPS
ICML
ML
JMLR
ArXiv:cs.LG

Related articles
Glossary of artificial intelligence
List of datasets for machine-learning research
Outline of machine learning
.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}.mw-parser-output .infobox .navbar{font-size:100%}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}vte
Fuzzy clustering (also referred to as soft clustering or soft k-means) is a form of clustering in which each data point can belong to more than one cluster.
Clustering or cluster analysis involves assigning data points to clusters such that items in the same cluster are as similar as possible, while items belonging to different clusters are as dissimilar as possible. Clusters are identified via similarity measures. These similarity measures include distance, connectivity, and intensity. Different similarity measures may be chosen based on the data or the application.[1]

Contents

1 Comparison to hard clustering
2 Membership
3 Fuzzy C-means clustering

3.1 History
3.2 General description
3.3 Centroid
3.4 Algorithm
3.5 Comparison to K-means clustering


4 Related algorithms
5 Example
6 Applications

6.1 Bioinformatics
6.2 Image analysis
6.3 Marketing


7 Image processing example
8 See also
9 References



Comparison to hard clustering[edit]
In non-fuzzy clustering (also known as hard clustering), data is divided into distinct clusters, where each data point can only belong to exactly one cluster. In fuzzy clustering, data points can potentially belong to multiple clusters. For example, an apple can be red or green (hard clustering), but an apple can also be red AND green (fuzzy clustering). Here, the apple can be red to a certain degree as well as green to a certain degree. Instead of the apple belonging to green [green = 1] and not red [red = 0], the apple can belong to green [green = 0.5] and red [red = 0.5]. These value are normalized between 0 and 1; however, they do not represent probabilities, so the two values do not need to add up to 1.

Membership[edit]
Membership grades are assigned to each of the data points (tags). These membership grades indicate the degree to which data points belong to each cluster. Thus, points on the edge of a cluster, with lower membership grades, may be in the cluster to a lesser degree than points in the center of cluster.

Fuzzy C-means clustering[edit]
One of the most widely used fuzzy clustering algorithms is the Fuzzy C-means clustering (FCM) algorithm.

History[edit]
Fuzzy c-means (FCM) clustering was developed by J.C. Dunn in 1973,[2] and improved by J.C. Bezdek in 1981.[3]

General description[edit]
The fuzzy c-means algorithm is very similar to the k-means algorithm:

Choose a number of clusters.
Assign coefficients randomly to each data point for being in the clusters.
Repeat until the algorithm has converged (that is, the coefficients' change between two iterations is no more than 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
, the given sensitivity threshold) :
Compute the centroid for each cluster (shown below).
For each data point, compute its coefficients of being in the clusters.
Centroid[edit]
Any point x has a set of coefficients giving the degree of being in the kth cluster wk(x). With fuzzy c-means, the centroid of a cluster is the mean of all points, weighted by their degree of belonging to the cluster, or, mathematically,

  
    
      
        
          c
          
            k
          
        
        =
        
          
            
              
                ∑
                
                  x
                
              
              
                
                  
                    w
                    
                      k
                    
                  
                  (
                  x
                  )
                
                
                  m
                
              
              x
            
            
              
                ∑
                
                  x
                
              
              
                
                  
                    w
                    
                      k
                    
                  
                  (
                  x
                  )
                
                
                  m
                
              
            
          
        
        ,
      
    
    {\displaystyle c_{k}={{\sum _{x}{w_{k}(x)}^{m}x} \over {\sum _{x}{w_{k}(x)}^{m}}},}
  

where m is the hyper- parameter that controls how fuzzy the cluster will be. The higher it is, the fuzzier the cluster will be in the end.

Algorithm[edit]
The FCM algorithm attempts to partition a finite collection of 
  
    
      
        n
      
    
    {\displaystyle n}
  
 elements 

  
    
      
        X
        =
        {
        
          
            x
          
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          
            x
          
          
            n
          
        
        }
      
    
    {\displaystyle X=\{\mathbf {x} _{1},...,\mathbf {x} _{n}\}}
  
 into a collection of c fuzzy clusters with respect to some given criterion.
Given a finite set of data, the algorithm returns a list of  
  
    
      
        c
      
    
    {\displaystyle c}
  
  cluster centres  
  
    
      
        C
        =
        {
        
          
            c
          
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          
            c
          
          
            c
          
        
        }
      
    
    {\displaystyle C=\{\mathbf {c} _{1},...,\mathbf {c} _{c}\}}
  
  and a partition matrix

  
    
      
        W
        =
        
          w
          
            i
            ,
            j
          
        
        ∈
        [
        0
        ,
        1
        ]
        ,
        
        i
        =
        1
        ,
        .
        .
        .
        ,
        n
        ,
        
        j
        =
        1
        ,
        .
        .
        .
        ,
        c
      
    
    {\displaystyle W=w_{i,j}\in [0,1],\;i=1,...,n,\;j=1,...,c}
  
, where each element, 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
 , tells
the degree to which element, 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
  
, belongs to cluster 
  
    
      
        
          
            c
          
          
            j
          
        
      
    
    {\displaystyle \mathbf {c} _{j}}
  
.
The FCM aims to minimize an objective function:


  
    
      
        
          
            
              a
              r
              g
              
              m
              i
              n
            
            C
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ∑
          
            j
            =
            1
          
          
            c
          
        
        
          w
          
            i
            j
          
          
            m
          
        
        
          
            ‖
            
              
                
                  x
                
                
                  i
                
              
              −
              
                
                  c
                
                
                  j
                
              
            
            ‖
          
          
            2
          
        
        ,
      
    
    {\displaystyle {\underset {C}{\operatorname {arg\,min} }}\sum _{i=1}^{n}\sum _{j=1}^{c}w_{ij}^{m}\left\|\mathbf {x} _{i}-\mathbf {c} _{j}\right\|^{2},}
  

where:


  
    
      
        
          w
          
            i
            j
          
        
        =
        
          
            1
            
              
                ∑
                
                  k
                  =
                  1
                
                
                  c
                
              
              
                
                  (
                  
                    
                      
                        ‖
                        
                          
                            
                              x
                            
                            
                              i
                            
                          
                          −
                          
                            
                              c
                            
                            
                              j
                            
                          
                        
                        ‖
                      
                      
                        ‖
                        
                          
                            
                              x
                            
                            
                              i
                            
                          
                          −
                          
                            
                              c
                            
                            
                              k
                            
                          
                        
                        ‖
                      
                    
                  
                  )
                
                
                  
                    2
                    
                      m
                      −
                      1
                    
                  
                
              
            
          
        
        .
      
    
    {\displaystyle w_{ij}={\frac {1}{\sum _{k=1}^{c}\left({\frac {\left\|\mathbf {x} _{i}-\mathbf {c} _{j}\right\|}{\left\|\mathbf {x} _{i}-\mathbf {c} _{k}\right\|}}\right)^{\frac {2}{m-1}}}}.}
  

Comparison to K-means clustering[edit]
K-means clustering also attempts to minimize the objective function shown above. This method differs from the k-means objective function by the addition of the membership values 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
 and the fuzzifier, 
  
    
      
        m
        ∈
        R
      
    
    {\displaystyle m\in R}
  
 ,  with  
  
    
      
        m
        ≥
        1
      
    
    {\displaystyle m\geq 1}
  
. The fuzzifier 
  
    
      
        m
      
    
    {\displaystyle m}
  
 determines the level of cluster fuzziness. A large 
  
    
      
        m
      
    
    {\displaystyle m}
  
 results in smaller membership values, 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
,  and hence, fuzzier clusters. In the limit 
  
    
      
        m
        =
        1
      
    
    {\displaystyle m=1}
  
, the memberships, 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
 , converge to 0 or 1, which implies a crisp partitioning. In the absence of experimentation or domain knowledge, 
  
    
      
        m
      
    
    {\displaystyle m}
  
 is commonly set to 2. The algorithm minimizes intra-cluster variance as well, but has the same problems as 'k'-means; the minimum is a local minimum, and the results depend on the initial choice of weights.

Related algorithms[edit]
Fuzzy C-means (FCM) with automatically determined for the number of clusters could enhance the detection accuracy.[4] Using a mixture of Gaussians along with the expectation-maximization algorithm is a more statistically formalized method which includes some of these ideas: partial membership in classes.

Example[edit]
To better understand this principle, a classic example of mono-dimensional data is given below on an x axis.


This data set can be traditionally grouped into two clusters. By selecting a threshold on the x-axis, the data is separated into two clusters.  The resulting clusters are labelled 'A' and 'B', as seen in the following image.  Each point belonging to the data set would therefore have a membership coefficient of 1 or 0. This membership coefficient of each corresponding data point is represented by the inclusion of the y-axis.   


In fuzzy clustering, each data point can have membership to multiple clusters.  By relaxing the definition of membership coefficients from strictly 1 or 0, these values can range from any value from 1 to 0. The following image shows the data set from the previous clustering, but now fuzzy c-means clustering is applied. First, a new threshold value defining two clusters may be generated. Next, new membership coefficients for each data point are generated based on clusters centroids, as well as distance from each cluster centroid.


As one can see, the middle data point belongs to cluster A and cluster B. the value of 0.3 is this data point's membership coefficient for cluster A .[5]

Applications[edit]
Clustering problems have applications in surface science, biology, medicine, psychology, economics, and many other disciplines.[6]

Bioinformatics[edit]
In the field of bioinformatics, clustering is used for a number of applications. One use is as a pattern recognition technique to analyze gene expression data from RNA-sequencing data or other technologies.[7] In this case, genes with similar expression patterns are grouped into the same cluster, and different clusters display distinct, well-separated patterns of expression. Use of clustering can provide insight into gene function and regulation.[6] Because fuzzy clustering allows genes to belong to more than one cluster, it allows for the identification of genes that are conditionally co-regulated or co-expressed.[8] For example, one gene may be acted on by more than one transcription factor, and one gene may encode a protein that has more than one function. Thus, fuzzy clustering is more appropriate than hard clustering.

Image analysis[edit]
Fuzzy c-means has been a very important tool for image processing in clustering objects in an image. In the 1970s, mathematicians introduced the spatial term into the FCM algorithm to improve the accuracy of clustering under noise.[9] Furthermore, FCM algorithms have been used to distinguish between different activities using image-based features such as the Hu and the Zernike Moments.[10] Alternatively, A fuzzy logic model can be described on fuzzy sets that are defined on three components of the HSL color space HSL and HSV; The membership functions aim to describe colors follow the human intuition of color identification.[11]

Marketing[edit]
In marketing, customers can be grouped into fuzzy clusters based on their needs, brand choices, psycho-graphic profiles, or other marketing related partitions.[citation needed]

Image processing example[edit]
  Image segmented by fuzzy clustering, with the original (top left), clustered (top right), and membership map (bottom)
Image segmentation using k-means clustering algorithms has long been used for pattern recognition, object detection, and medical imaging. However, due to real world limitations such as noise, shadowing, and variations in cameras, traditional hard clustering is often unable to reliably perform image processing tasks as stated above.[citation needed]  Fuzzy clustering has been proposed as a more applicable algorithm in the performance to these tasks.  Given is gray scale image that has undergone fuzzy clustering in Matlab.[12]  The original image is seen next to a clustered image.  Colors are used to give a visual representation of the three distinct clusters used to identify the membership of each pixel. Below, a chart is given that defines the fuzzy membership coefficients of their corresponding intensity values.
Depending on the application for which the fuzzy clustering coefficients are to be used, different pre-processing techniques can be applied to RGB images.  RGB to HCL conversion is common practice.[13]

See also[edit]
FLAME Clustering
Cluster Analysis
Expectation-maximization algorithm (a similar, but more statistically formalized method)
References[edit]
.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}

^ .mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}"Fuzzy Clustering". reference.wolfram.com. Retrieved 2016-04-26.

^ Dunn, J. C. (1973-01-01). "A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact Well-Separated Clusters". Journal of Cybernetics. 3 (3): 32–57. doi:10.1080/01969727308546046. ISSN 0022-0280.

^ Bezdek, James C. (1981). Pattern Recognition with Fuzzy Objective Function Algorithms. ISBN 0-306-40671-3.

^ Said, E El-Khamy; Rowayda A Sadek; Mohamed A El-Khoreby (October 2015). "An efficient brain mass detection with adaptive clustered based fuzzy C-mean and thresholding". 2015 IEEE International Conference on Signal and Image Processing Applications (ICSIPA): 429–433.

^ "Clustering - Fuzzy C-means". home.deib.polimi.it. Retrieved 2017-05-01.

^ a b Ben-Dor, Amir; Shamir, Ron; Yakhini, Zohar (1999-10-01). "Clustering Gene Expression Patterns". Journal of Computational Biology. 6 (3–4): 281–297. CiteSeerX 10.1.1.34.5341. doi:10.1089/106652799318274. ISSN 1066-5277. PMID 10582567.

^ Valafar, Faramarz (2002-12-01). "Pattern Recognition Techniques in Microarray Data Analysis". Annals of the New York Academy of Sciences. 980 (1): 41–64. CiteSeerX 10.1.1.199.6445. doi:10.1111/j.1749-6632.2002.tb04888.x. ISSN 1749-6632. PMID 12594081.

^ Valafar F. Pattern recognition techniques in microarray data analysis. Annals of the New York Academy of Sciences. 2002 Dec 1;980(1):41-64.

^ Ahmed, Mohamed N.; Yamany, Sameh M.; Mohamed, Nevin; Farag, Aly A.; Moriarty, Thomas (2002). "A Modified Fuzzy C-Means Algorithm for Bias Field Estimation and Segmentation of MRI Data" (PDF). IEEE Transactions on Medical Imaging. 21 (3): 193–199. CiteSeerX 10.1.1.331.9742. doi:10.1109/42.996338. PMID 11989844..

^ Banerjee, Tanvi (2014). "Day or Night Activity Recognition From Video Using Fuzzy Clustering Techniques". IEEE Transactions on Fuzzy Systems. 22 (3): 483–493. CiteSeerX 10.1.1.652.2819. doi:10.1109/TFUZZ.2013.2260756.

^ Alireza, Kashani; Kashani, Amir; Milani, Nargess; Akhlaghi, Peyman; Khezri, Kaveh (2008). Robust Color Classification Using Fuzzy Reasoning and Genetic Algorithms in RoboCup Soccer Leagues. Robocup. Lecture Notes in Computer Science. 5001. pp. 548–555. doi:10.1007/978-3-540-68847-1_59. ISBN 978-3-540-68846-4.

^ "Fuzzy Clustering - MATLAB & Simulink". www.mathworks.com. Retrieved 2017-05-03.

^ Lecca, Paola (2011). Systemic Approaches in Bioinformatics and Computational Systems Biology. IGI Global. p. 9. ISBN 9781613504369.







Retrieved from "https://en.wikipedia.org/w/index.php?title=Fuzzy_clustering&oldid=1059871842"
		Categories: Cluster analysis algorithmsHidden categories: All articles with unsourced statementsArticles with unsourced statements from March 2020Articles with unsourced statements from December 2021
	